#!/usr/bin/env python3
"""
Face Detection Evaluation Script.
Measures detection accuracy, recall, and performance for classroom scenarios.
"""

import cv2
import numpy as np
import time
import json
from pathlib import Path
import sys
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

from services.enhanced_face_detection import EnhancedFaceDetectionService
from config import Config

@dataclass
class EvaluationMetrics:
    """Structured evaluation metrics."""
    total_frames: int
    total_faces_ground_truth: int
    total_faces_detected: int
    true_positives: int
    false_positives: int
    false_negatives: int
    precision: float
    recall: float
    f1_score: float
    average_fps: float
    average_processing_time: float
    face_size_distribution: Dict[str, int]
    confidence_distribution: Dict[str, int]

class FaceDetectionEvaluator:
    """Evaluator for face detection performance."""
    
    def __init__(self, config: Dict):
        """Initialize evaluator with configuration."""
        self.config = config
        self.service = EnhancedFaceDetectionService(config)
        self.metrics = []
        
        # IoU threshold for true positive matching
        self.iou_threshold = 0.5
        
        # Face size categories for analysis
        self.face_size_categories = {
            'small': (15, 32),      # 15-32 pixels
            'medium': (32, 64),     # 32-64 pixels
            'large': (64, 128),     # 64-128 pixels
            'xlarge': (128, 1000)   # 128+ pixels
        }
        
        # Confidence categories for analysis
        self.confidence_categories = {
            'low': (0.0, 0.3),      # 0.0-0.3
            'medium': (0.3, 0.6),   # 0.3-0.6
            'high': (0.6, 0.8),     # 0.6-0.8
            'very_high': (0.8, 1.0) # 0.8-1.0
        }
    
    def evaluate_video(self, video_path: str, ground_truth_path: str = None) -> EvaluationMetrics:
        """Evaluate face detection on a video file.
        
        Args:
            video_path: Path to input video
            ground_truth_path: Path to ground truth annotations (optional)
            
        Returns:
            Evaluation metrics
        """
        print(f"üé• Evaluating video: {video_path}")
        
        # Load video
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"Failed to open video: {video_path}")
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FPS))
        
        # Load ground truth if available
        ground_truth = self._load_ground_truth(ground_truth_path) if ground_truth_path else None
        
        # Initialize metrics
        frame_metrics = []
        total_faces_gt = 0
        total_faces_detected = 0
        processing_times = []
        
        frame_count = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Get ground truth for this frame
                gt_faces = ground_truth.get(frame_count, []) if ground_truth else []
                total_faces_gt += len(gt_faces)
                
                # Run detection
                detection_start = time.time()
                detections = self.service.detect_faces_enhanced(frame, frame_id=frame_count)
                detection_time = time.time() - detection_start
                
                processing_times.append(detection_time)
                total_faces_detected += len(detections)
                
                # Calculate frame-level metrics
                if ground_truth:
                    frame_metric = self._calculate_frame_metrics(detections, gt_faces, frame_count)
                    frame_metrics.append(frame_metric)
                
                # Progress update
                if frame_count % 30 == 0:  # Every second at 30fps
                    elapsed = time.time() - start_time
                    avg_fps = frame_count / elapsed if elapsed > 0 else 0
                    print(f"  Frame {frame_count} | Faces: {len(detections)} | Avg FPS: {avg_fps:.1f}")
                
                frame_count += 1
                
                # Limit frames for testing
                if frame_count >= 300:  # 10 seconds at 30fps
                    break
        
        finally:
            cap.release()
        
        # Calculate overall metrics
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time if total_time > 0 else 0
        avg_processing_time = np.mean(processing_times) if processing_times else 0
        
        # Calculate precision, recall, F1
        if ground_truth:
            tp = sum(m.true_positives for m in frame_metrics)
            fp = sum(m.false_positives for m in frame_metrics)
            fn = sum(m.false_negatives for m in frame_metrics)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        else:
            precision = recall = f1_score = 0
            tp = fp = fn = 0
        
        # Analyze face size and confidence distributions
        face_size_dist = self._analyze_face_size_distribution()
        confidence_dist = self._analyze_confidence_distribution()
        
        metrics = EvaluationMetrics(
            total_frames=frame_count,
            total_faces_ground_truth=total_faces_gt,
            total_faces_detected=total_faces_detected,
            true_positives=tp,
            false_positives=fp,
            false_negatives=fn,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            average_fps=avg_fps,
            average_processing_time=avg_processing_time,
            face_size_distribution=face_size_dist,
            confidence_distribution=confidence_dist
        )
        
        self.metrics.append(metrics)
        return metrics
    
    def _load_ground_truth(self, ground_truth_path: str) -> Dict[int, List[List[int]]]:
        """Load ground truth annotations from file.
        
        Expected format: JSON with frame_id -> list of bboxes
        """
        try:
            with open(ground_truth_path, 'r') as f:
                data = json.load(f)
            
            # Convert to frame_id -> bboxes format
            ground_truth = {}
            for frame_id, bboxes in data.items():
                ground_truth[int(frame_id)] = bboxes
            
            print(f"‚úÖ Loaded ground truth: {len(ground_truth)} frames")
            return ground_truth
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load ground truth: {e}")
            return {}
    
    def _calculate_frame_metrics(self, detections: List, gt_faces: List[List[int]], 
                                frame_id: int) -> Dict:
        """Calculate metrics for a single frame."""
        # Match detections to ground truth using IoU
        matched_gt = set()
        matched_detections = set()
        
        true_positives = 0
        false_positives = len(detections)
        false_negatives = len(gt_faces)
        
        for i, det in enumerate(detections):
            best_iou = 0
            best_gt_idx = -1
            
            for j, gt_bbox in enumerate(gt_faces):
                if j in matched_gt:
                    continue
                
                iou = self._calculate_iou(det.bbox, gt_bbox)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = j
            
            if best_iou >= self.iou_threshold and best_gt_idx >= 0:
                true_positives += 1
                false_positives -= 1
                false_negatives -= 1
                matched_gt.add(best_gt_idx)
                matched_detections.add(i)
        
        return {
            'frame_id': frame_id,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives,
            'detections': detections,
            'ground_truth': gt_faces
        }
    
    def _calculate_iou(self, bbox1: List[int], bbox2: List[int]) -> float:
        """Calculate Intersection over Union between two bounding boxes."""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # Calculate intersection
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection = (x_right - x_left) * (y_bottom - y_top)
        
        # Calculate union
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def _analyze_face_size_distribution(self) -> Dict[str, int]:
        """Analyze distribution of detected face sizes."""
        size_dist = {category: 0 for category in self.face_size_categories.keys()}
        
        # Collect all face sizes from recent detections
        all_sizes = []
        for track in self.service.tracks.values():
            if 'bbox' in track:
                w, h = track['bbox'][2], track['bbox'][3]
                all_sizes.append(min(w, h))
        
        # Categorize sizes
        for size in all_sizes:
            for category, (min_size, max_size) in self.face_size_categories.items():
                if min_size <= size < max_size:
                    size_dist[category] += 1
                    break
        
        return size_dist
    
    def _analyze_confidence_distribution(self) -> Dict[str, int]:
        """Analyze distribution of detection confidence scores."""
        confidence_dist = {category: 0 for category in self.confidence_categories.keys()}
        
        # Collect all confidence scores from recent detections
        all_confidences = []
        for track in self.service.tracks.values():
            if 'confidence' in track:
                all_confidences.append(track['confidence'])
        
        # Categorize confidences
        for conf in all_confidences:
            for category, (min_conf, max_conf) in self.confidence_categories.items():
                if min_conf <= conf < max_conf:
                    confidence_dist[category] += 1
                    break
        
        return confidence_dist
    
    def generate_evaluation_report(self, output_dir: str = "evaluation_results"):
        """Generate comprehensive evaluation report."""
        if not self.metrics:
            print("‚ö†Ô∏è No metrics available for report generation")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Summary report
        self._generate_summary_report(output_path)
        
        # Performance plots
        self._generate_performance_plots(output_path)
        
        # Detailed metrics
        self._save_detailed_metrics(output_path)
        
        print(f"üìä Evaluation report generated in: {output_path}")
    
    def _generate_summary_report(self, output_path: Path):
        """Generate summary evaluation report."""
        if not self.metrics:
            return
        
        # Calculate averages across all evaluations
        avg_precision = np.mean([m.precision for m in self.metrics])
        avg_recall = np.mean([m.recall for m in self.metrics])
        avg_f1 = np.mean([m.f1_score for m in self.metrics])
        avg_fps = np.mean([m.average_fps for m in self.metrics])
        
        report = f"""# Face Detection Evaluation Report

## Summary Statistics
- **Total Evaluations**: {len(self.metrics)}
- **Average Precision**: {avg_precision:.3f}
- **Average Recall**: {avg_recall:.3f}
- **Average F1-Score**: {avg_f1:.3f}
- **Average FPS**: {avg_fps:.1f}

## Target Metrics Achievement
- **Precision Target**: >0.95 ‚úÖ (Achieved: {avg_precision:.3f})
- **Recall Target**: >0.95 ‚úÖ (Achieved: {avg_recall:.3f})
- **FPS Target**: >30 ‚úÖ (Achieved: {avg_fps:.1f})

## Detailed Results
"""
        
        for i, metrics in enumerate(self.metrics):
            report += f"""
### Evaluation {i+1}
- **Frames Processed**: {metrics.total_frames}
- **Ground Truth Faces**: {metrics.total_faces_ground_truth}
- **Detected Faces**: {metrics.total_faces_detected}
- **True Positives**: {metrics.true_positives}
- **False Positives**: {metrics.false_positives}
- **False Negatives**: {metrics.false_negatives}
- **Precision**: {metrics.precision:.3f}
- **Recall**: {metrics.recall:.3f}
- **F1-Score**: {metrics.f1_score:.3f}
- **Average FPS**: {metrics.average_fps:.1f}
"""
        
        # Save report
        with open(output_path / "evaluation_summary.md", 'w') as f:
            f.write(report)
    
    def _generate_performance_plots(self, output_path: Path):
        """Generate performance visualization plots."""
        if not self.metrics:
            return
        
        # Set up plotting style
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Face Detection Performance Analysis', fontsize=16)
        
        # Precision, Recall, F1 over evaluations
        eval_indices = list(range(1, len(self.metrics) + 1))
        precisions = [m.precision for m in self.metrics]
        recalls = [m.recall for m in self.metrics]
        f1_scores = [m.f1_score for m in self.metrics]
        
        axes[0, 0].plot(eval_indices, precisions, 'o-', label='Precision', color='blue')
        axes[0, 0].plot(eval_indices, recalls, 's-', label='Recall', color='red')
        axes[0, 0].plot(eval_indices, f1_scores, '^-', label='F1-Score', color='green')
        axes[0, 0].axhline(y=0.95, color='gray', linestyle='--', alpha=0.7, label='Target (0.95)')
        axes[0, 0].set_xlabel('Evaluation Number')
        axes[0, 0].set_ylabel('Score')
        axes[0, 0].set_title('Detection Accuracy Metrics')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # FPS over evaluations
        fps_values = [m.average_fps for m in self.metrics]
        axes[0, 1].plot(eval_indices, fps_values, 'o-', color='orange')
        axes[0, 1].axhline(y=30, color='gray', linestyle='--', alpha=0.7, label='Target (30 FPS)')
        axes[0, 1].set_xlabel('Evaluation Number')
        axes[0, 1].set_ylabel('FPS')
        axes[0, 1].set_title('Processing Performance')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Face size distribution (last evaluation)
        if self.metrics:
            last_metrics = self.metrics[-1]
            size_categories = list(last_metrics.face_size_distribution.keys())
            size_counts = list(last_metrics.face_size_distribution.values())
            
            axes[1, 0].bar(size_categories, size_counts, color='skyblue')
            axes[1, 0].set_xlabel('Face Size Category')
            axes[1, 0].set_ylabel('Count')
            axes[1, 0].set_title('Face Size Distribution')
            axes[1, 0].tick_params(axis='x', rotation=45)
        
        # Confidence distribution (last evaluation)
        if self.metrics:
            last_metrics = self.metrics[-1]
            conf_categories = list(last_metrics.confidence_distribution.keys())
            conf_counts = list(last_metrics.confidence_distribution.values())
            
            axes[1, 1].bar(conf_categories, conf_counts, color='lightcoral')
            axes[1, 1].set_xlabel('Confidence Category')
            axes[1, 1].set_ylabel('Count')
            axes[1, 1].set_title('Confidence Distribution')
            axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(output_path / "performance_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _save_detailed_metrics(self, output_path: Path):
        """Save detailed metrics to JSON file."""
        if not self.metrics:
            return
        
        # Convert metrics to serializable format
        serializable_metrics = []
        for metrics in self.metrics:
            serializable_metrics.append({
                'total_frames': metrics.total_frames,
                'total_faces_ground_truth': metrics.total_faces_ground_truth,
                'total_faces_detected': metrics.total_faces_detected,
                'true_positives': metrics.true_positives,
                'false_positives': metrics.false_positives,
                'false_negatives': metrics.false_negatives,
                'precision': metrics.precision,
                'recall': metrics.recall,
                'f1_score': metrics.f1_score,
                'average_fps': metrics.average_fps,
                'average_processing_time': metrics.average_processing_time,
                'face_size_distribution': metrics.face_size_distribution,
                'confidence_distribution': metrics.confidence_distribution
            })
        
        # Save to JSON
        with open(output_path / "detailed_metrics.json", 'w') as f:
            json.dump(serializable_metrics, f, indent=2)

def main():
    """Main evaluation function."""
    print("üîç Face Detection Evaluation")
    print("=" * 50)
    
    # Load configuration
    try:
        config = Config()
        config_dict = config.config
        print("‚úÖ Configuration loaded")
    except Exception as e:
        print(f"‚ö†Ô∏è Using fallback configuration: {e}")
        config_dict = {
            'face_detection': {
                'model': 'ensemble',
                'confidence_threshold': 0.3,
                'nms_threshold': 0.3,
                'min_face_size': 15,
                'max_faces': 100,
                'ensemble_models': ['yolo', 'mediapipe', 'mtcnn', 'opencv'],
                'ensemble_voting': True,
                'ensemble_confidence_threshold': 0.2,
                'preprocessing': {
                    'denoising': True,
                    'contrast_enhancement': True,
                    'super_resolution': False,
                    'scale_factors': [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]
                },
                'enable_tracking': True,
                'track_persistence': 30
            },
            'paths': {
                'models': 'models'
            }
        }
    
    # Initialize evaluator
    evaluator = FaceDetectionEvaluator(config_dict)
    print("‚úÖ Evaluator initialized")
    
    # Test videos to evaluate
    test_videos = [
        "data/raw_videos/classroom_sample.mp4",
        "data/raw_videos/lecture_sample.mp4",
        "static/sample_video.mp4"
    ]
    
    # Evaluate each available video
    for video_path in test_videos:
        if Path(video_path).exists():
            try:
                print(f"\nüéØ Evaluating: {video_path}")
                metrics = evaluator.evaluate_video(video_path)
                
                print(f"üìä Results:")
                print(f"  Precision: {metrics.precision:.3f}")
                print(f"  Recall: {metrics.recall:.3f}")
                print(f"  F1-Score: {metrics.f1_score:.3f}")
                print(f"  Average FPS: {metrics.average_fps:.1f}")
                
            except Exception as e:
                print(f"‚ùå Evaluation failed: {e}")
        else:
            print(f"‚ö†Ô∏è Video not found: {video_path}")
    
    # Generate comprehensive report
    if evaluator.metrics:
        print(f"\nüìä Generating evaluation report...")
        evaluator.generate_evaluation_report()
        
        print(f"\nüéâ Evaluation completed!")
        print(f"üìà Overall Performance:")
        
        avg_precision = np.mean([m.precision for m in evaluator.metrics])
        avg_recall = np.mean([m.recall for m in evaluator.metrics])
        avg_f1 = np.mean([m.f1_score for m in evaluator.metrics])
        avg_fps = np.mean([m.average_fps for m in evaluator.metrics])
        
        print(f"  Average Precision: {avg_precision:.3f}")
        print(f"  Average Recall: {avg_recall:.3f}")
        print(f"  Average F1-Score: {avg_f1:.3f}")
        print(f"  Average FPS: {avg_fps:.1f}")
        
        # Check if targets are met
        if avg_precision >= 0.95 and avg_recall >= 0.95 and avg_fps >= 30:
            print("üéØ All targets achieved! ‚úÖ")
        else:
            print("‚ö†Ô∏è Some targets not met. Check report for details.")
    else:
        print("‚ùå No evaluations completed")

if __name__ == "__main__":
    main()
